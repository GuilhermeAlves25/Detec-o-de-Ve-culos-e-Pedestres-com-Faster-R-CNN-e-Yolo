{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_Oqj22Gd-z"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparação do ambiente"
      ],
      "metadata": {
        "id": "yp1kl-_OQR1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5JDMkXBR1v9"
      },
      "outputs": [],
      "source": [
        "# Instalar Ultralytics para a comparação com YOLO\n",
        "!pip install ultralytics -q\n",
        "\n",
        "# Importar as bibliotecas necessárias\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "# Importações do PyTorch\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Configurar o dispositivo para GPU se disponível\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Usando dispositivo: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCtm5ksGEiSw"
      },
      "source": [
        "Montar Drive e Copiar Dataset Original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Gi6UYoScmi"
      },
      "outputs": [],
      "source": [
        "# Célula 2: Montar Drive e Copiar Dataset Original\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print(\"Conectando ao Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Caminhos de ORIGEM (Google Drive)\n",
        "drive_project_path = '/content/drive/MyDrive/Projeto_IA_KITTI_Clean'\n",
        "source_image_dir = os.path.join(drive_project_path, 'training', 'image_2')\n",
        "source_label_dir = os.path.join(drive_project_path, 'training', 'label_2')\n",
        "\n",
        "# Caminhos de DESTINO para a cópia local dos dados ORIGINAIS\n",
        "local_original_image_dir = '/content/training_original/image_2'\n",
        "local_original_label_dir = '/content/training_original/label_2'\n",
        "\n",
        "print(\"\\nCopiando dataset original do Drive para o ambiente local...\")\n",
        "!rm -rf /content/training_original\n",
        "!mkdir -p {local_original_image_dir}\n",
        "!mkdir -p {local_original_label_dir}\n",
        "!cp {source_image_dir}/* {local_original_image_dir}\n",
        "!cp {source_label_dir}/* {local_original_label_dir}\n",
        "\n",
        "print(\"Cópia dos dados originais concluída!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuNWwsfqE5UZ"
      },
      "source": [
        "Pré-processar Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iztCjBIsH1z5"
      },
      "outputs": [],
      "source": [
        "# Célula 3: Pré-processar Dados\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "\n",
        "# Caminhos de origem (a cópia local que acabou de ser feita)\n",
        "source_image_dir = '/content/training_original/image_2'\n",
        "source_label_dir = '/content/training_original/label_2'\n",
        "\n",
        "# Caminhos de destino para o novo dataset PRÉ-PROCESSADO\n",
        "processed_image_dir = '/content/kitti_processed/image_2'\n",
        "processed_label_dir = '/content/kitti_processed/label_2'\n",
        "\n",
        "TARGET_SIZE = (600, 600)\n",
        "os.makedirs(processed_image_dir, exist_ok=True)\n",
        "os.makedirs(processed_label_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Iniciando pré-processamento. Imagens serão salvas em: {processed_image_dir}\")\n",
        "image_files = os.listdir(source_image_dir)\n",
        "\n",
        "for img_name in tqdm(image_files, desc=\"Pré-processando imagens e anotações\"):\n",
        "    img_id = img_name.split('.')[0]\n",
        "    img_path_orig = os.path.join(source_image_dir, img_name)\n",
        "    label_path_orig = os.path.join(source_label_dir, f\"{img_id}.txt\")\n",
        "\n",
        "    image = cv2.imread(img_path_orig)\n",
        "    if image is None: continue # Pula imagens corrompidas\n",
        "    original_h, original_w, _ = image.shape\n",
        "    resized_image = cv2.resize(image, TARGET_SIZE)\n",
        "    cv2.imwrite(os.path.join(processed_image_dir, img_name), resized_image)\n",
        "\n",
        "    new_labels = []\n",
        "    with open(label_path_orig, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            class_name = parts[0]\n",
        "            other_parts = parts[1:4] + parts[8:]\n",
        "            xmin, ymin, xmax, ymax = map(float, parts[4:8])\n",
        "            xmin_scaled = xmin * (TARGET_SIZE[0] / original_w); xmax_scaled = xmax * (TARGET_SIZE[0] / original_w)\n",
        "            ymin_scaled = ymin * (TARGET_SIZE[1] / original_h); ymax_scaled = ymax * (TARGET_SIZE[1] / original_h)\n",
        "            new_line = f\"{class_name} {' '.join(other_parts[:3])} {xmin_scaled:.2f} {ymin_scaled:.2f} {xmax_scaled:.2f} {ymax_scaled:.2f} {' '.join(other_parts[3:])}\"\n",
        "            new_labels.append(new_line)\n",
        "    with open(os.path.join(processed_label_dir, f\"{img_id}.txt\"), 'w') as f:\n",
        "        f.write(\"\\n\".join(new_labels))\n",
        "\n",
        "print(\"\\n--- Pré-processamento concluído! ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc6pb1F9IAEC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6WBzTpEFE3g"
      },
      "source": [
        "Definir Dataset e Criar DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFe5LHj1S1tw"
      },
      "outputs": [],
      "source": [
        "# Célula 4: Criar Dataset e DataLoaders\n",
        "\n",
        "# --- ATUALIZANDO VARIÁVEIS DE CAMINHO PARA USAR DADOS PROCESSADOS ---\n",
        "IMAGE_PATH = '/content/kitti_processed/image_2'\n",
        "LABEL_PATH = '/content/kitti_processed/label_2'\n",
        "\n",
        "# --- CLASSE DE DATASET SIMPLIFICADA ---\n",
        "class KittiDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, class_mapping, transforms=None):\n",
        "        self.image_dir = image_dir; self.label_dir = label_dir; self.class_mapping = class_mapping\n",
        "        self.transforms = transforms\n",
        "        self.image_ids = sorted([f.split('.')[0] for f in os.listdir(image_dir)])\n",
        "    def __len__(self): return len(self.image_ids)\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_path = os.path.join(self.image_dir, f\"{img_id}.png\")\n",
        "        label_path = os.path.join(self.label_dir, f\"{img_id}.txt\")\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "        boxes, labels = [], []\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(); class_name = parts[0]\n",
        "                if class_name in self.class_mapping:\n",
        "                    labels.append(self.class_mapping[class_name])\n",
        "                    boxes.append(list(map(float, parts[4:8])))\n",
        "        target = {\"boxes\": torch.as_tensor(boxes, dtype=torch.float32), \"labels\": torch.as_tensor(labels, dtype=torch.int64)}\n",
        "        if self.transforms: image = self.transforms(image)\n",
        "        return image, target\n",
        "\n",
        "# --- CRIAÇÃO DATASETS E DATALOADERS ---\n",
        "CLASSES = ['background', 'Car', 'Pedestrian', 'Cyclist']\n",
        "CLASS_MAPPING = {'Car': 1, 'Pedestrian': 2, 'Cyclist': 3}\n",
        "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "full_dataset = KittiDataset(IMAGE_PATH, LABEL_PATH, CLASS_MAPPING, transforms=transforms)\n",
        "\n",
        "print(f\"\\nUsando o dataset completo com {len(full_dataset)} imagens.\")\n",
        "\n",
        "# A divisão de 80/20 agora é feita em cima do dataset completo\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "def collate_fn(batch): return tuple(zip(*batch))\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"DataLoaders prontos: {len(train_dataset)} treino, {len(val_dataset)} validação.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z2LYD1ZkQxe"
      },
      "source": [
        "Apagar checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVAgPU_MkSou"
      },
      "outputs": [],
      "source": [
        "# Célula para limpar checkpoints antigos\n",
        "!rm frcnn_kitti_epoch_*.pth\n",
        "print(\"Checkpoints antigos foram removidos.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv7Z7_kNFPYQ"
      },
      "source": [
        " Treinamento do FASTER R-CNN com backbone ResNet-50 pré-treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw5RsGOYFN7M"
      },
      "outputs": [],
      "source": [
        "# Célula de Treinamento (com Checkpoints)\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# --- DEFINIÇÃO DO MODELO ---\n",
        "def get_model(num_classes):\n",
        "    # Carrega um modelo Faster R-CNN com backbone ResNet-50 pré-treinado\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # Substitui a camada final para o número de classes do dataset\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# Instancia o modelo e o move para a GPU\n",
        "model_frcnn = get_model(len(CLASSES)).to(device)\n",
        "\n",
        "# --- OTIMIZADOR E CONFIGURAÇÕES DE TREINO ---\n",
        "params = [p for p in model_frcnn.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Definição do numero de épocas\n",
        "num_epochs = 5\n",
        "start_epoch = 0\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_to_load = 'frcnn_kitti_epoch_1.pth'\n",
        "\n",
        "if os.path.exists(checkpoint_to_load):\n",
        "    print(f\"Carregando checkpoint: {checkpoint_to_load}...\")\n",
        "    model_frcnn.load_state_dict(torch.load(checkpoint_to_load, map_location=device))\n",
        "    try:\n",
        "        # Extrai o número da época do nome do arquivo para continuar de onde parou\n",
        "        start_epoch = int(checkpoint_to_load.split('_')[-1].split('.')[0])\n",
        "        print(f\"Checkpoint carregado. O treinamento continuará a partir da época {start_epoch + 1}.\")\n",
        "    except:\n",
        "        print(\"Não foi possível extrair o número da época. O treino começará da época 1 com os pesos carregados.\")\n",
        "        start_epoch = 0\n",
        "else:\n",
        "    print(\"Nenhum checkpoint encontrado. Iniciando o treinamento do zero.\")\n",
        "\n",
        "\n",
        "# --- LOOP DE TREINAMENTO ---\n",
        "print(\"\\n--- INICIANDO TREINAMENTO DO FASTER R-CNN ---\")\n",
        "=\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model_frcnn.train() # Coloca o modelo em modo de treino\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Época {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for images, targets in progress_bar:\n",
        "        # Mover dados para a GPU\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Calcular a perda (loss)\n",
        "        loss_dict = model_frcnn(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += losses.item()\n",
        "        progress_bar.set_postfix(loss=losses.item())\n",
        "\n",
        "    print(f\"Fim da Época {epoch+1}, Perda Média: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # --- SALVANDO O CHECKPOINT NO FINAL DE CADA ÉPOCA ---\n",
        "    checkpoint_path = f'frcnn_kitti_epoch_{epoch+1}.pth'\n",
        "    torch.save(model_frcnn.state_dict(), checkpoint_path)\n",
        "    print(f\"Checkpoint da Época {epoch+1} salvo em '{checkpoint_path}'\")\n",
        "\n",
        "print(\"\\n--- TREINAMENTO DO FASTER R-CNN CONCLUÍDO ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KXLtQNQUirU"
      },
      "source": [
        "Treinamento para o Faster R-CNN com Backbone MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO7eGQdIUjW8"
      },
      "outputs": [],
      "source": [
        "# Célula de Treinamento (com Backbone MobileNet)\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# --- DEFINIÇÃO DO MODELO COM BACKBONE MOBILENET ---\n",
        "def get_mobilenet_model(num_classes):\n",
        "\n",
        "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "model_mobilenet = get_mobilenet_model(len(CLASSES)).to(device)\n",
        "\n",
        "# --- OTIMIZADOR E CONFIGURAÇÕES DE TREINO ---\n",
        "params = [p for p in model_mobilenet.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Treinar por menos épocas, apenas para comparação de velocidade e performance\n",
        "num_epochs = 5\n",
        "start_epoch = 0\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_basename = 'mobilenet_kitti'\n",
        "checkpoint_to_load = f'{checkpoint_basename}_epoch_1.pth'\n",
        "\n",
        "if os.path.exists(checkpoint_to_load):\n",
        "    print(f\"Carregando checkpoint: {checkpoint_to_load}...\")\n",
        "    model_mobilenet.load_state_dict(torch.load(checkpoint_to_load, map_location=device))\n",
        "    try:\n",
        "        start_epoch = int(checkpoint_to_load.split('_')[-1].split('.')[0])\n",
        "        print(f\"Checkpoint carregado. O treinamento continuará a partir da época {start_epoch + 1}.\")\n",
        "    except:\n",
        "        print(\"Não foi possível extrair o número da época. O treino começará da época 1 com os pesos carregados.\")\n",
        "        start_epoch = 0\n",
        "else:\n",
        "    print(\"Nenhum checkpoint do MobileNet encontrado. Iniciando o treinamento do zero.\")\n",
        "\n",
        "\n",
        "# --- LOOP DE TREINAMENTO (USANDO O MODELO MOBILENET) ---\n",
        "print(f\"\\\\n--- INICIANDO TREINAMENTO DO FASTER R-CNN COM BACKBONE MOBILENET ---\")\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model_mobilenet.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Época {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for images, targets in progress_bar:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model_mobilenet(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += losses.item()\n",
        "        progress_bar.set_postfix(loss=losses.item())\n",
        "\n",
        "    print(f\"Fim da Época {epoch+1}, Perda Média: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # --- SALVANDO O CHECKPOINT (COM NOME DIFERENTE) ---\n",
        "    checkpoint_path = f'{checkpoint_basename}_epoch_{epoch+1}.pth'\n",
        "    torch.save(model_mobilenet.state_dict(), checkpoint_path)\n",
        "    print(f\"Checkpoint da Época {epoch+1} salvo em '{checkpoint_path}'\")\n",
        "\n",
        "print(\"\\n--- TREINAMENTO DO FASTER R-CNN COM MOBILENET CONCLUÍDO ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teaVICromugN"
      },
      "source": [
        "CÉLULA DE AVALIAÇÃO (APENAS PARA O MODELO COM MOBILENET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5gxgGummvVN"
      },
      "outputs": [],
      "source": [
        "# CÉLULA DE AVALIAÇÃO (APENAS PARA O MODELO COM MOBILENET)\n",
        "\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. CARREGAR O MODELO MOBILENET TREINADO --- # <-- MUDANÇA IMPORTANTE\n",
        "# Certifique-se de que o nome do arquivo corresponde ao checkpoint final do seu treino com MobileNet\n",
        "# Se você treinou por 5 épocas, o nome do arquivo será 'mobilenet_kitti_epoch_5.pth'\n",
        "print(\"Carregando pesos do modelo com MobileNet...\")\n",
        "model_mobilenet.load_state_dict(torch.load('mobilenet_kitti_epoch_5.pth', map_location=device))\n",
        "\n",
        "\n",
        "# --- 2. AVALIAÇÃO ---\n",
        "# A lógica é a mesma, mas usamos a variável 'model_mobilenet'\n",
        "metric = MeanAveragePrecision(box_format='xyxy')\n",
        "\n",
        "model_mobilenet.eval() # <-- MUDANÇA\n",
        "print(\"\\nIniciando avaliação do modelo MobileNet no conjunto de validação...\")\n",
        "progress_bar = tqdm(val_loader, desc=\"Avaliando MobileNet\") # <-- MUDANÇA\n",
        "with torch.no_grad():\n",
        "    for images, targets in progress_bar:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        predictions = model_mobilenet(images) # <-- MUDANÇA\n",
        "        formatted_targets = [{\"boxes\": t[\"boxes\"].to(device), \"labels\": t[\"labels\"].to(device)} for t in targets]\n",
        "        metric.update(predictions, formatted_targets)\n",
        "\n",
        "# 3. Calcular e exibir os resultados finais\n",
        "print(\"\\n--- Resultados da Avaliação (Faster R-CNN com MobileNet) ---\") # <-- MUDANÇA\n",
        "results = metric.compute()\n",
        "\n",
        "# Imprimir os resultados gerais\n",
        "print(f\"mAP (IoU=0.50:0.95): {results['map'].item():.4f}\")\n",
        "print(f\"mAP (IoU=0.50):      {results['map_50'].item():.4f}\")\n",
        "print(f\"mAP (IoU=0.75):      {results['map_75'].item():.4f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Recall Médio:        {results['mar_100'].item():.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Impressão de resultados por classe (robusto)\n",
        "class_ids = np.atleast_1d(results.get('classes', torch.tensor([])).cpu().numpy())\n",
        "map_per_class = np.atleast_1d(results.get('map_per_class', torch.tensor([])).cpu().numpy())\n",
        "recall_per_class = np.atleast_1d(results.get('mar_100_per_class', torch.tensor([])).cpu().numpy())\n",
        "class_map_results = {cid: val for cid, val in zip(class_ids, map_per_class)}\n",
        "class_recall_results = {cid: val for cid, val in zip(class_ids, recall_per_class)}\n",
        "\n",
        "for i, class_name in enumerate(CLASSES):\n",
        "    if i == 0: continue\n",
        "    class_id = i\n",
        "    map_val = class_map_results.get(class_id, \"N/A\")\n",
        "    recall_val = class_recall_results.get(class_id, \"N/A\")\n",
        "    map_str = f\"{map_val:.4f}\" if isinstance(map_val, (np.floating, float)) else map_val\n",
        "    recall_str = f\"{recall_val:.4f}\" if isinstance(recall_val, (np.floating, float)) else recall_val\n",
        "    print(f\"Classe: {class_name} (ID: {class_id})\")\n",
        "    print(f\"  - AP @ .50:.95: {map_str}\")\n",
        "    print(f\"  - Recall:      {recall_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsHcCYXybCRu"
      },
      "source": [
        "Avaliação Numérica backbone ResNet-50 (Métricas Oficiais: AP, Recall, Precisão)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNgbaBhrbDBG"
      },
      "outputs": [],
      "source": [
        "# Célula de Avaliação de Métricas\n",
        "\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np # Importar numpy\n",
        "\n",
        "# 1. Inicializar a métrica\n",
        "metric = MeanAveragePrecision(box_format='xyxy')\n",
        "\n",
        "# 2. Loop de avaliação (sem alterações)\n",
        "model_frcnn.eval()\n",
        "print(\"Iniciando avaliação no conjunto de validação...\")\n",
        "progress_bar = tqdm(val_loader, desc=\"Avaliando\")\n",
        "with torch.no_grad():\n",
        "    for images, targets in progress_bar:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        predictions = model_frcnn(images)\n",
        "        formatted_targets = [{\"boxes\": t[\"boxes\"].to(device), \"labels\": t[\"labels\"].to(device)} for t in targets]\n",
        "        metric.update(predictions, formatted_targets)\n",
        "\n",
        "# 3. Calcular e exibir os resultados finais\n",
        "print(\"\\n--- Resultados da Avaliação ---\")\n",
        "results = metric.compute()\n",
        "\n",
        "# Imprimir os resultados gerais\n",
        "print(f\"mAP (IoU=0.50:0.95): {results['map'].item():.4f}\")\n",
        "print(f\"mAP (IoU=0.50):      {results['map_50'].item():.4f}\")\n",
        "print(f\"mAP (IoU=0.75):      {results['map_75'].item():.4f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Recall Médio:        {results['mar_100'].item():.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- IMPRESSÃO DE  RESULTADOS POR CLASSE ---\n",
        "# Usamos np.atleast_1d para garantir que o resultado seja sempre iterável\n",
        "class_ids = np.atleast_1d(results.get('classes', torch.tensor([])).cpu().numpy())\n",
        "map_per_class = np.atleast_1d(results.get('map_per_class', torch.tensor([])).cpu().numpy())\n",
        "recall_per_class = np.atleast_1d(results.get('mar_100_per_class', torch.tensor([])).cpu().numpy())\n",
        "\n",
        "# Mapeamos o ID da classe ao resultado para busca fácil\n",
        "class_map_results = {cid: val for cid, val in zip(class_ids, map_per_class)}\n",
        "class_recall_results = {cid: val for cid, val in zip(class_ids, recall_per_class)}\n",
        "\n",
        "# Iteração sobre nossas classes conhecidas e buscamos o resultado no dicionário\n",
        "for i, class_name in enumerate(CLASSES):\n",
        "    if i == 0: continue # Pular background\n",
        "\n",
        "    class_id = i\n",
        "    map_val = class_map_results.get(class_id, \"N/A\")\n",
        "    recall_val = class_recall_results.get(class_id, \"N/A\")\n",
        "\n",
        "    map_str = f\"{map_val:.4f}\" if isinstance(map_val, (np.floating, float)) else map_val\n",
        "    recall_str = f\"{recall_val:.4f}\" if isinstance(recall_val, (np.floating, float)) else recall_val\n",
        "\n",
        "    print(f\"Classe: {class_name} (ID: {class_id})\")\n",
        "    print(f\"  - AP @ .50:.95: {map_str}\")\n",
        "    print(f\"  - Recall:      {recall_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliação Visual do Modelo Treinado"
      ],
      "metadata": {
        "id": "j_EBsu2GRjQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula para Avaliação Visual do Modelo Treinado\n",
        "\n",
        "# Colocar o modelo em modo de avaliação (importante para resultados consistentes)\n",
        "model_frcnn.eval()\n",
        "\n",
        "# Pegar um lote de imagens do CONJUNTO DE VALIDAÇÃO\n",
        "images, targets = next(iter(val_loader))\n",
        "images = list(img.to(device) for img in images)\n",
        "\n",
        "# Fazer as previsões com o seu modelo treinado\n",
        "with torch.no_grad():\n",
        "    predictions = model_frcnn(images)\n",
        "\n",
        "print(\"Mostrando resultados em algumas imagens de validação...\")\n",
        "print(\"Caixas VERDES = Previsão do Modelo | Caixas VERMELHAS = Gabarito Real\")\n",
        "\n",
        "# Mover as imagens de volta para a CPU para poder desenhar nelas\n",
        "images_cpu = [img.cpu() for img in images]\n",
        "\n",
        "# Loop para visualizar cada imagem e suas detecções\n",
        "for i in range(len(images_cpu)):\n",
        "    # Converter o tensor da imagem para um formato que o OpenCV entende\n",
        "    img_tensor = images_cpu[i]\n",
        "    img_np = img_tensor.permute(1, 2, 0).numpy()\n",
        "\n",
        "\n",
        "    # Multiplicamos por 255 para reverter a normalização e ter cores corretas\n",
        "    img_to_draw = np.ascontiguousarray(img_np * 255).astype(np.uint8)\n",
        "\n",
        "    # 1. Desenhar as caixas previstas pelo modelo (em VERDE)\n",
        "    prediction = predictions[i]\n",
        "    for box, score, label in zip(prediction['boxes'], prediction['scores'], prediction['labels']):\n",
        "        if score > 0.6: # Limiar de confiança (só desenhar previsões confiáveis)\n",
        "            box = box.cpu().numpy().astype(int)\n",
        "            class_name = CLASSES[label.item()]\n",
        "\n",
        "            # Desenha a caixa\n",
        "            cv2.rectangle(img_to_draw, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "            # Escreve o texto com a classe e a confiança\n",
        "            cv2.putText(img_to_draw, f\"{class_name}: {score:.2f}\", (box[0], box[1]-10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # 2. Desenhar as caixas do gabarito real (em VERMELHO) para comparação\n",
        "    true_boxes = targets[i]['boxes'].cpu().numpy().astype(int)\n",
        "    for box in true_boxes:\n",
        "        cv2.rectangle(img_to_draw, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 1) # Linha mais fina\n",
        "\n",
        "    # 3. Mostrar a imagem\n",
        "    plt.figure(figsize=(12, 9))\n",
        "    plt.imshow(img_to_draw)\n",
        "    plt.title(f\"Resultado na Imagem de Validação {i+1}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rD54kXmZRiNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANÁLISE DE CASOS ESPECÍFICOS (ILUMINAÇÃO E OCLUSÃO)"
      ],
      "metadata": {
        "id": "83P8ExPeSNrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model_frcnn.eval()\n",
        "\n",
        "\n",
        "imagens_noturnas_ou_sombra = ['000048', '000077', '000140', '000401', '000222', '000241', '000329']\n",
        "imagens_com_oclusao = ['000223', '000308', '000338', '000360', '000466', '000000473', '000587'] # Imagens com objetos parcialmente escondidos\n",
        "imagens_diurnas_boas = ['000335', '000584', '000627', '000678', '000764', '000772', '000787'] # Imagens \"fáceis\" para comparação\n",
        "\n",
        "\n",
        "def analisar_imagens(model, image_ids, analysis_title):\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(f\"  {analysis_title}\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Caixas VERDES = Previsão do Modelo | Caixas VERMELHAS = Gabarito Real\")\n",
        "\n",
        "    for img_id in image_ids:\n",
        "        img_path = os.path.join(IMAGE_PATH, f\"{img_id}.png\")\n",
        "        # Encontrar o target (gabarito) correspondente no val_dataset\n",
        "        target_original = None\n",
        "        for img, tgt in val_dataset:\n",
        "\n",
        "            pass\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        image_tensor = torchvision.transforms.ToTensor()(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model([image_tensor])[0]\n",
        "\n",
        "        img_to_draw = image.copy()\n",
        "\n",
        "        # Desenhar previsões (VERDE)\n",
        "        for box, score, label in zip(prediction['boxes'], prediction['scores'], prediction['labels']):\n",
        "            if score > 0.6:\n",
        "                box = box.cpu().numpy().astype(int)\n",
        "                class_name = CLASSES[label.item()]\n",
        "                cv2.rectangle(img_to_draw, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
        "                cv2.putText(img_to_draw, f\"{class_name}: {score:.2f}\", (box[0], box[1]-10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Desenhar gabarito (VERMELHO) - lendo o arquivo de anotação original\n",
        "        label_path = os.path.join(LABEL_PATH, f\"{img_id}.txt\")\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if parts[0] in CLASS_MAPPING:\n",
        "                    box = np.array(list(map(float, parts[4:8]))).astype(int)\n",
        "                    cv2.rectangle(img_to_draw, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 1)\n",
        "\n",
        "        plt.figure(figsize=(12, 9))\n",
        "        plt.imshow(cv2.cvtColor(img_to_draw, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f\"ID: {img_id}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# --- 4. EXECUTAR A ANÁLISE PARA CADA CATEGORIA ---\n",
        "analisar_imagens(model_frcnn, imagens_diurnas_boas, \"Análise em Condições Diurnas (Boas)\")\n",
        "analisar_imagens(model_frcnn, imagens_noturnas_ou_sombra, \"Análise em Condições de Baixa Iluminação / Sombra\")\n",
        "analisar_imagens(model_frcnn, imagens_com_oclusao, \"Análise de Desempenho em Oclusão\")"
      ],
      "metadata": {
        "id": "1QuHI7kWSF7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NcnYWdG0bDm"
      },
      "source": [
        "Preparação dos Dados para o YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XgbenM51Nat"
      },
      "outputs": [],
      "source": [
        "# PREPARAÇÃO DE DADOS DO YOLO\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "\n",
        "# --- 1. DEFINIR CAMINHOS ---\n",
        "# Caminho dos dados ORIGINAIS (copiados do seu Drive para o ambiente local)\n",
        "original_images_path = '/content/training_original/image_2' # <-- CORRIGIDO\n",
        "original_labels_path = '/content/training_original/label_2' # <-- CORRIGIDO\n",
        "\n",
        "# Caminho de destino para a nova estrutura de pastas do YOLO\n",
        "yolo_path = '/content/KITTI_YOLO' # Salvaremos em /content/ no Colab\n",
        "yolo_train_img_dir = os.path.join(yolo_path, 'images', 'train')\n",
        "yolo_val_img_dir = os.path.join(yolo_path, 'images', 'val')\n",
        "yolo_train_label_dir = os.path.join(yolo_path, 'labels', 'train')\n",
        "yolo_val_label_dir = os.path.join(yolo_path, 'labels', 'val')\n",
        "\n",
        "# Limpar e recriar as pastas de destino\n",
        "!rm -rf {yolo_path}\n",
        "os.makedirs(yolo_train_img_dir, exist_ok=True); os.makedirs(yolo_val_img_dir, exist_ok=True)\n",
        "os.makedirs(yolo_train_label_dir, exist_ok=True); os.makedirs(yolo_val_label_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- 2. CRIAR A DIVISÃO TREINO/VALIDAÇÃO ---\n",
        "# Pega todos os IDs de imagem, embaralha e divide 80/20\n",
        "print(\"Criando divisão de dados para treino e validação...\")\n",
        "all_image_ids = sorted([f.split('.')[0] for f in os.listdir(original_images_path)])\n",
        "random.seed(42)\n",
        "random.shuffle(all_image_ids)\n",
        "split_idx = int(0.8 * len(all_image_ids))\n",
        "train_image_ids = all_image_ids[:split_idx]\n",
        "val_image_ids = all_image_ids[split_idx:]\n",
        "print(f\"Divisão criada: {len(train_image_ids)} treino, {len(val_image_ids)} validação.\")\n",
        "\n",
        "\n",
        "# --- 3. FUNÇÃO DE CONVERSÃO ---\n",
        "def convert_yolo_files(image_ids, subset_name):\n",
        "    print(f\"Convertendo {len(image_ids)} arquivos para o conjunto de {subset_name} do YOLO...\")\n",
        "    img_dest_path = os.path.join(yolo_path, 'images', subset_name)\n",
        "    label_dest_path = os.path.join(yolo_path, 'labels', subset_name)\n",
        "\n",
        "    for img_id in tqdm(image_ids, desc=f\"Processando {subset_name}\"):\n",
        "        img_name = f\"{img_id}.png\"\n",
        "        src_img_path = os.path.join(original_images_path, img_name)\n",
        "        src_label_path = os.path.join(original_labels_path, f\"{img_id}.txt\")\n",
        "\n",
        "        if not os.path.exists(src_img_path) or not os.path.exists(src_label_path):\n",
        "            continue\n",
        "\n",
        "        shutil.copy(src_img_path, os.path.join(img_dest_path, img_name))\n",
        "        img = cv2.imread(os.path.join(img_dest_path, img_name))\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not read image file {img_name}. Skipping.\")\n",
        "            continue\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "\n",
        "        with open(src_label_path, 'r') as f_in, open(os.path.join(label_dest_path, f\"{img_id}.txt\"), 'w') as f_out:\n",
        "            for line in f_in:\n",
        "                parts = line.strip().split()\n",
        "                class_name = parts[0]\n",
        "                if class_name in CLASS_MAPPING:\n",
        "                    class_id = CLASS_MAPPING[class_name]\n",
        "                    # KITTI format is xmin, ymin, xmax, ymax\n",
        "                    xmin, ymin, xmax, ymax = map(float, parts[4:8])\n",
        "\n",
        "                    # Convert to YOLO format (x_center, y_center, width, height) normalized\n",
        "                    x_center = ((xmin + xmax) / 2) / w\n",
        "                    y_center = ((ymin + ymax) / 2) / h\n",
        "                    bbox_w = (xmax - xmin) / w\n",
        "                    bbox_h = (ymax - ymin) / h\n",
        "\n",
        "                    # YOLO classes are 0-indexed (Car=0, Pedestrian=1, etc.)\n",
        "                    # Our classes are 1-indexed (Car=1, etc.), so subtract 1.\n",
        "                    yolo_class_id = class_id - 1\n",
        "\n",
        "                    # Ensure coordinates are within [0, 1] and format as strings\n",
        "                    x_center_str = f\"{max(0.0, min(1.0, x_center)):.6f}\"\n",
        "                    y_center_str = f\"{max(0.0, min(1.0, y_center)):.6f}\"\n",
        "                    bbox_w_str = f\"{max(0.0, min(1.0, bbox_w)):.6f}\"\n",
        "                    bbox_h_str = f\"{max(0.0, min(1.0, bbox_h)):.6f}\"\n",
        "\n",
        "\n",
        "                    # Write the line in YOLO format: class_id x_center y_center width height\n",
        "                    f_out.write(f\"{yolo_class_id} {x_center_str} {y_center_str} {bbox_w_str} {bbox_h_str}\\n\")\n",
        "\n",
        "\n",
        "# --- 4. EXECUTAR A CONVERSION ---\n",
        "convert_yolo_files(train_image_ids, \"train\")\n",
        "convert_yolo_files(val_image_ids, \"val\")\n",
        "\n",
        "# --- 5. FINAL VERIFICATION ---\n",
        "num_train_images = len(os.listdir(yolo_train_img_dir))\n",
        "print(f\"\\nVerification: {num_train_images} images were copied to the YOLO train folder.\")\n",
        "if num_train_images == 0:\n",
        "    raise RuntimeError(\"The YOLO train folder is empty. The copy script failed.\")\n",
        "\n",
        "# --- 6. CREATE dataset.yaml file ---\n",
        "yaml_content = f\"\"\"\n",
        "path: {os.path.abspath(yolo_path)}\n",
        "train: images/train\n",
        "val: images/val\n",
        "names:\n",
        "  0: Car\n",
        "  1: Pedestrian\n",
        "  2: Cyclist\n",
        "\"\"\"\n",
        "yaml_path = os.path.join(yolo_path, 'dataset.yaml')\n",
        "with open(yaml_path, 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(f\"\\nYOLO preparation completed. Configuration file saved to: {yaml_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvb_aE8S2WP3"
      },
      "source": [
        "Treinamento do Modelo YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcDe2Sz62q51"
      },
      "outputs": [],
      "source": [
        "# CÉLULA DE TREINAMENTO DO YOLO (AJUSTADA)\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Carregar o modelo YOLOv8 pré-treinado\n",
        "model_yolo = YOLO('yolov8n.pt')\n",
        "\n",
        "# Iniciar o treinamento\n",
        "print(\"\\n--- INICIANDO TREINAMENTO DO YOLOv8 ---\")\n",
        "model_yolo.train(\n",
        "    data=yaml_path,      # Caminho para o arquivo .yaml que a célula anterior criou\n",
        "    epochs=20,           # Usar 20 épocas para uma comparação justa com o Faster R-CNN\n",
        "    imgsz=600,           # <-- MUDANÇA PRINCIPAL: Ajustado para corresponder ao pré-processamento\n",
        "    project='kitti_results',\n",
        "    name='yolov8n_comparison'\n",
        ")\n",
        "\n",
        "print(\"\\n--- TREINAMENTO DO YOLOv8 CONCLUÍDO ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula para encontrar o caminho correto do treinamento do YOLO\n",
        "!ls kitti_results/"
      ],
      "metadata": {
        "id": "vzf8rqsDMK91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliação Numérica do YOLO"
      ],
      "metadata": {
        "id": "2TXXtCLyJpay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA DE AVALIAÇÃO NUMÉRICA DO YOLO\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Carregar o melhor modelo salvo pelo treinamento\n",
        "best_yolo_model = YOLO('kitti_results/yolov8n_comparison13/weights/best.pt')\n",
        "\n",
        "# Executar a validação para obter as métricas\n",
        "print(\"\\n--- Resultados da Avaliação (YOLOv8) ---\")\n",
        "results = best_yolo_model.val()"
      ],
      "metadata": {
        "id": "xZ8Sgv3nJqG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análise Visual de Casos Específicos (YOLO)"
      ],
      "metadata": {
        "id": "x9su6aSrJs6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA DE ANÁLISE DE CASOS ESPECÍFICOS (YOLO)\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Suas listas de imagens para análise (USE AS MESMAS DA ANÁLISE DO FASTER R-CNN) ---\n",
        "# Lembre-se que estas listas contêm apenas os IDs dos arquivos (ex: '000118')\n",
        "imagens_noturnas_ou_sombra = ['000048', '000077', '000140', '000401', '000222', '000241', '000329']\n",
        "imagens_com_oclusao = ['000223', '000308', '000338', '000360', '000466', '000000473', '000587'] # Imagens com objetos parcialmente escondidos\n",
        "imagens_diurnas_boas = ['000335', '000584', '000627', '000678', '000764', '000772', '000787'] # Imagens \"fáceis\" para comparação\n",
        "\n",
        "# Carregar o melhor modelo\n",
        "best_yolo_model = YOLO('kitti_results/yolov8n_comparison13/weights/best.pt')\n",
        "\n",
        "def analisar_imagens_yolo(model, image_ids, analysis_title):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"  {analysis_title} (YOLOv8)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Caminho para as imagens originais que foram copiadas para o ambiente\n",
        "    original_images_path = '/content/training_original/image_2'\n",
        "\n",
        "    # Filtrar apenas os IDs que realmente existem\n",
        "    image_paths = []\n",
        "    for img_id in image_ids:\n",
        "        path = os.path.join(original_images_path, f\"{img_id}.png\")\n",
        "        if os.path.exists(path):\n",
        "            image_paths.append(path)\n",
        "\n",
        "    if not image_paths:\n",
        "        print(f\"Nenhuma imagem encontrada para a análise: {analysis_title}\")\n",
        "        return\n",
        "\n",
        "    # Fazer a predição em todas as imagens de uma vez\n",
        "    predictions = model.predict(source=image_paths)\n",
        "\n",
        "    # Mostrar os resultados\n",
        "    for result in predictions:\n",
        "        im_array = result.plot() # .plot() já desenha as caixas na imagem\n",
        "        plt.figure(figsize=(12, 9))\n",
        "        plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f\"Resultado do YOLOv8 em: {os.path.basename(result.path)}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# Executar a análise visual\n",
        "analisar_imagens_yolo(best_yolo_model, imagens_diurnas_boas, \"Análise em Condições Diurnas (Boas)\")\n",
        "analisar_imagens_yolo(best_yolo_model, imagens_noturnas_ou_sombra, \"Análise em Condições de Baixa Iluminação / Sombra\")\n",
        "analisar_imagens_yolo(best_yolo_model, imagens_com_oclusao, \"Análise de Desempenho em Oclusão\")"
      ],
      "metadata": {
        "id": "dvse6BHPJ6NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_k4aAyqE6ls"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNdHGX48z7Xi"
      },
      "outputs": [],
      "source": [
        "# Célula para Contar a Distribuição das Classes\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Dicionário reverso para mapear ID de volta para o nome da classe\n",
        "inv_class_mapping = {v: k for k, v in CLASS_MAPPING.items()}\n",
        "\n",
        "# Contadores para cada conjunto de dados\n",
        "train_counts = Counter()\n",
        "val_counts = Counter()\n",
        "\n",
        "print(\"Analisando o conjunto de treinamento...\")\n",
        "for _, targets in tqdm(train_dataset):\n",
        "    # .tolist() converte os tensores para uma lista de números\n",
        "    labels = targets['labels'].tolist()\n",
        "    train_counts.update(labels)\n",
        "\n",
        "print(\"\\nAnalisando o conjunto de validação...\")\n",
        "for _, targets in tqdm(val_dataset):\n",
        "    labels = targets['labels'].tolist()\n",
        "    val_counts.update(labels)\n",
        "\n",
        "print(\"\\n--- Distribuição de Classes ---\")\n",
        "print(\"\\nConjunto de Treinamento:\")\n",
        "for label_id, count in sorted(train_counts.items()):\n",
        "    print(f\"  - Classe '{inv_class_mapping[label_id]}': {count} objetos\")\n",
        "\n",
        "print(\"\\nConjunto de Validação:\")\n",
        "for label_id, count in sorted(val_counts.items()):\n",
        "    print(f\"  - Classe '{inv_class_mapping[label_id]}': {count} objetos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaWlyIuFiETo"
      },
      "source": [
        "PROPOSTAS DA RPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpvM-6-ciHII"
      },
      "outputs": [],
      "source": [
        "# CÉLULA PARA VISUALIZAR AS PROPOSTAS DA RPN\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Importar numpy\n",
        "\n",
        "# --- carregamento do melhor checkpoint treinado ---\n",
        "model_frcnn.load_state_dict(torch.load('frcnn_kitti_epoch_20.pth', map_location=device))\n",
        "model_frcnn.to(device)\n",
        "model_frcnn.eval()\n",
        "\n",
        "# Pegar uma imagem de exemplo do seu val_loader\n",
        "# Ajuste o índice [0] para visualizar outras imagens\n",
        "img_tensor, target = val_dataset[0] # Pegar o primeiro item do dataset de validação\n",
        "img_for_pred = img_tensor.to(device) # Remove unsqueeze(0) here\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    # 1. Obter o mapa de características do backbone\n",
        "    images = [img_for_pred] # Transform espera uma list a de imagens\n",
        "    # Ensure images are on the correct device - this is already done above\n",
        "    images, targets = model_frcnn.transform(images, None)\n",
        "\n",
        "\n",
        "    features = model_frcnn.backbone(images.tensors)\n",
        "\n",
        "    # 2. Obter as propostas da RPN\n",
        "    # A RPN retorna as caixas propostas e suas perdas (que ignoramos na inferência)\n",
        "    proposals, proposal_losses = model_frcnn.rpn(images, features, None)\n",
        "\n",
        "# Pegar as caixas propostas para a primeira imagem e mover para a CPU\n",
        "rpn_boxes = proposals[0].cpu().numpy()\n",
        "\n",
        "# --- Visualização ---\n",
        "# Converter a imagem original para um formato que o OpenCV entende\n",
        "img_cv = img_tensor.permute(1, 2, 0).cpu().numpy().copy() * 255\n",
        "img_cv = cv2.cvtColor(img_cv.astype('uint8'), cv2.COLOR_RGB2BGR) # Corrigido: cv2.COLOR_RGB2BGR\n",
        "\n",
        "print(f\"A RPN gerou {len(rpn_boxes)} propostas de região para esta imagem.\")\n",
        "print(\"Desenhando as 100 propostas com maior pontuação...\")\n",
        "\n",
        "# Desenhar as 100 melhores propostas na imagem\n",
        "# A RPN não dá pontuações de classe, mas sim uma pontuação de \"objeto vs não-objeto\".\n",
        "for i, box in enumerate(rpn_boxes[:100]):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    # Desenha um retângulo amarelo semi-transparente\n",
        "    cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 255), 1)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
        "plt.title(\"Visualização das Propostas da Region Proposal Network (RPN)\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}